{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping process...\n",
      "Found 1 new articles. Saving to kompas_bencana4.xlsx...\n",
      "File updated with new articles.\n",
      "Displaying the latest scraped articles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://surabaya.kompas.com/read/2025/02/20/110...</td>\n",
       "      <td>20/02/2025</td>\n",
       "      <td>Gunung Semeru Erupsi 5 Kali, Letusan Asap Capa...</td>\n",
       "      <td>Tim Redaksi  LUMAJANG, KOMPAS.com - Gunung Sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://denpasar.kompas.com/read/2025/02/19/160...</td>\n",
       "      <td>19/02/2025</td>\n",
       "      <td>Ratusan Keluarga di Buleleng Kesulitan Air Ber...</td>\n",
       "      <td>Tim Redaksi  BULELENG, KOMPAS.com - Ratusan ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/18/201...</td>\n",
       "      <td>18/02/2025</td>\n",
       "      <td>Tunggu 2 Bulan, Warga 4 Desa Terdampak Erupsi ...</td>\n",
       "      <td>Tim Redaksi  FLORES TIMUR, KOMPAS.com – Penjab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/14/170...</td>\n",
       "      <td>14/02/2025</td>\n",
       "      <td>Korupsi Bansos Korban Banjir Bandang di Lembat...</td>\n",
       "      <td>Tim Redaksi  LEMBATA, KOMPAS.com – Kejaksaan N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://money.kompas.com/read/2025/02/14/073000...</td>\n",
       "      <td>14/02/2025</td>\n",
       "      <td>PetroChina Bantu Penanggulangan Bencana di Sek...</td>\n",
       "      <td>Editor  JAKARTA, KOMPAS.com – PetroChina Inter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL        Date  \\\n",
       "7  http://surabaya.kompas.com/read/2025/02/20/110...  20/02/2025   \n",
       "0  http://denpasar.kompas.com/read/2025/02/19/160...  19/02/2025   \n",
       "1  http://regional.kompas.com/read/2025/02/18/201...  18/02/2025   \n",
       "2  http://regional.kompas.com/read/2025/02/14/170...  14/02/2025   \n",
       "3  http://money.kompas.com/read/2025/02/14/073000...  14/02/2025   \n",
       "\n",
       "                                               Title  \\\n",
       "7  Gunung Semeru Erupsi 5 Kali, Letusan Asap Capa...   \n",
       "0  Ratusan Keluarga di Buleleng Kesulitan Air Ber...   \n",
       "1  Tunggu 2 Bulan, Warga 4 Desa Terdampak Erupsi ...   \n",
       "2  Korupsi Bansos Korban Banjir Bandang di Lembat...   \n",
       "3  PetroChina Bantu Penanggulangan Bencana di Sek...   \n",
       "\n",
       "                                             Content  \n",
       "7  Tim Redaksi  LUMAJANG, KOMPAS.com - Gunung Sem...  \n",
       "0  Tim Redaksi  BULELENG, KOMPAS.com - Ratusan ke...  \n",
       "1  Tim Redaksi  FLORES TIMUR, KOMPAS.com – Penjab...  \n",
       "2  Tim Redaksi  LEMBATA, KOMPAS.com – Kejaksaan N...  \n",
       "3  Editor  JAKARTA, KOMPAS.com – PetroChina Inter...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 5.0 minutes before the next run...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "def live_scrape_kompas(file_name=\"kompas_bencana4.xlsx\", interval=300):\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk melakukan scraping artikel bencana dari halaman Kompas dan menyimpan hasilnya ke file Excel.\n",
    "    Fungsi ini berjalan secara live dengan interval waktu yang ditentukan.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): Nama file Excel untuk menyimpan hasil scraping.\n",
    "    - interval (int): Waktu tunggu antar scraping dalam detik (default: 300 detik atau 5 menit).\n",
    "    \"\"\"\n",
    "    # Base URL\n",
    "    url = \"https://www.kompas.com/tag/bencana?page=1\"\n",
    "\n",
    "    # Header untuk request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Pola kategori\n",
    "    category_patterns = {\n",
    "        \"Banjir\": r\"\\bbanjir\\b|\\bbanjir bandang\\b\",\n",
    "        \"Gempa Bumi\": r\"\\bgempa\\b|\\bgempa bumi\\b\",\n",
    "        \"Tanah Longsor\": r\"\\btanah longsor\\b|\\blongsor\\b\",\n",
    "        \"Gunung Meletus\": r\"\\bgunung meletus\\b|\\berupsi\\b\",\n",
    "        \"Tsunami\": r\"\\btsunami\\b\",\n",
    "        \"Puting Beliung\": r\"\\bputing beliung\\b|\\bbadai\\b\",\n",
    "        \"Kekeringan\": r\"\\bkekeringan\\b|\\bkemarau panjang\\b\",\n",
    "        \"Cuaca Ekstrem\": r\"\\bcuaca ekstrem\\b|\\bhujan es\\b|\\bgelombang panas\\b\",\n",
    "        \"Gelombang Ekstrem\": r\"\\bgelombang ekstrem\\b|\\bgemuruh laut\\b\",\n",
    "        \"Kebakaran Hutan\": r\"\\bkebakaran hutan\\b|\\bapi di hutan\\b|\\bkarhutla\\b\"\n",
    "    }\n",
    "\n",
    "    # Gabungkan semua pola menjadi satu regex\n",
    "    keyword_pattern = re.compile(\"|\".join(category_patterns.values()), re.IGNORECASE)\n",
    "\n",
    "    def scrape_page():\n",
    "        \"\"\"Scrapes the first page of Kompas tag 'bencana' for articles.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Gagal mengakses halaman: {e}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='article__list__title')\n",
    "        scraped_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Judul dan URL\n",
    "                title = article.find('h3', class_='article__title').get_text(strip=True)\n",
    "                link = article.find('a', class_='article__link')['href']\n",
    "                date = article.find_next_sibling('div', class_='article__list__info').find('div', class_='article__date').get_text(strip=True)\n",
    "                date = date.split(',')[0]  # Ambil bagian sebelum koma\n",
    "\n",
    "                # Request konten artikel\n",
    "                article_response = requests.get(link, headers=headers)\n",
    "                if article_response.status_code != 200:\n",
    "                    print(f\"Gagal mengakses artikel: {link}\")\n",
    "                    continue\n",
    "\n",
    "                article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "                # Ambil semua paragraf\n",
    "                paragraphs = []\n",
    "                for p in article_soup.find_all('p'):\n",
    "                    if p.find_parent('div', class_='footerCopyright'):\n",
    "                        continue\n",
    "                    if not p.has_attr('class'):\n",
    "                        text = ''.join([elem.strip() if isinstance(elem, str) else ' ' + elem.get_text(strip=True) + ' ' for elem in p.contents])\n",
    "                        paragraphs.append(text)\n",
    "                content = ' '.join(paragraphs)\n",
    "                content = re.sub(r'^(Tim Redaksi -|Editor -)\\s*', '', content)\n",
    "\n",
    "                # Filter hanya artikel dengan kategori\n",
    "                if keyword_pattern.search(title) or keyword_pattern.search(content):\n",
    "                    category = \"Lainnya\"\n",
    "                    for cat, pattern in category_patterns.items():\n",
    "                        if re.search(pattern, content, re.IGNORECASE) or re.search(pattern, title, re.IGNORECASE):\n",
    "                            category = cat\n",
    "                            break\n",
    "\n",
    "                    scraped_data.append({'URL': link, 'Date': date, 'Title': title, 'Content': content})\n",
    "            except Exception as e:\n",
    "                print(f\"Error pada artikel: {e}\")\n",
    "        return scraped_data\n",
    "\n",
    "    # Loop scraping secara berkala\n",
    "    while True:\n",
    "        print(\"Starting scraping process...\")\n",
    "        scraped_articles = scrape_page()\n",
    "\n",
    "        if scraped_articles:\n",
    "            if os.path.exists(file_name):\n",
    "                df_existing = pd.read_excel(file_name)\n",
    "                existing_urls = set(df_existing['URL'])\n",
    "            else:\n",
    "                df_existing = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "                existing_urls = set()\n",
    "\n",
    "            new_articles = [article for article in scraped_articles if article['URL'] not in existing_urls]\n",
    "            if new_articles:\n",
    "                print(f\"Found {len(new_articles)} new articles. Saving to {file_name}...\")\n",
    "                df_new = pd.DataFrame(new_articles)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                df_combined.to_excel(file_name, index=False)\n",
    "                print(f\"File updated with new articles.\")\n",
    "            else:\n",
    "                print(\"No new articles found.\")\n",
    "        else:\n",
    "            print(\"No articles found during scraping.\")\n",
    "\n",
    "        # Menampilkan data terbaru di tabel\n",
    "        print(\"Displaying the latest scraped articles:\")\n",
    "        df_sorted = pd.read_excel(file_name).sort_values(by=\"Date\", ascending=False)\n",
    "        display(df_sorted.head())\n",
    "\n",
    "        print(f\"Waiting for {interval / 60} minutes before the next run...\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Jalankan fungsi\n",
    "live_scrape_kompas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://denpasar.kompas.com/read/2025/02/19/160...</td>\n",
       "      <td>19/02/2025</td>\n",
       "      <td>Ratusan Keluarga di Buleleng Kesulitan Air Ber...</td>\n",
       "      <td>Tim Redaksi  BULELENG, KOMPAS.com - Ratusan ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/18/201...</td>\n",
       "      <td>18/02/2025</td>\n",
       "      <td>Tunggu 2 Bulan, Warga 4 Desa Terdampak Erupsi ...</td>\n",
       "      <td>Tim Redaksi  FLORES TIMUR, KOMPAS.com – Penjab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/14/170...</td>\n",
       "      <td>14/02/2025</td>\n",
       "      <td>Korupsi Bansos Korban Banjir Bandang di Lembat...</td>\n",
       "      <td>Tim Redaksi  LEMBATA, KOMPAS.com – Kejaksaan N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://money.kompas.com/read/2025/02/14/073000...</td>\n",
       "      <td>14/02/2025</td>\n",
       "      <td>PetroChina Bantu Penanggulangan Bencana di Sek...</td>\n",
       "      <td>Editor  JAKARTA, KOMPAS.com – PetroChina Inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/13/151...</td>\n",
       "      <td>13/02/2025</td>\n",
       "      <td>Buaya 4 Meter Muncul Saat Banjir di Makassar, ...</td>\n",
       "      <td>Tim Redaksi  MAKASSAR, KOMPAS.com - Seekor bua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL        Date  \\\n",
       "0  http://denpasar.kompas.com/read/2025/02/19/160...  19/02/2025   \n",
       "1  http://regional.kompas.com/read/2025/02/18/201...  18/02/2025   \n",
       "2  http://regional.kompas.com/read/2025/02/14/170...  14/02/2025   \n",
       "3  http://money.kompas.com/read/2025/02/14/073000...  14/02/2025   \n",
       "4  http://regional.kompas.com/read/2025/02/13/151...  13/02/2025   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Ratusan Keluarga di Buleleng Kesulitan Air Ber...   \n",
       "1  Tunggu 2 Bulan, Warga 4 Desa Terdampak Erupsi ...   \n",
       "2  Korupsi Bansos Korban Banjir Bandang di Lembat...   \n",
       "3  PetroChina Bantu Penanggulangan Bencana di Sek...   \n",
       "4  Buaya 4 Meter Muncul Saat Banjir di Makassar, ...   \n",
       "\n",
       "                                             Content  \n",
       "0  Tim Redaksi  BULELENG, KOMPAS.com - Ratusan ke...  \n",
       "1  Tim Redaksi  FLORES TIMUR, KOMPAS.com – Penjab...  \n",
       "2  Tim Redaksi  LEMBATA, KOMPAS.com – Kejaksaan N...  \n",
       "3  Editor  JAKARTA, KOMPAS.com – PetroChina Inter...  \n",
       "4  Tim Redaksi  MAKASSAR, KOMPAS.com - Seekor bua...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"kompas_bencana4.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
