{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time as t\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrasi Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Data Gabungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi pembantu untuk mengupdate file gabungan\n",
    "def update_gabungan(new_articles, gabungan_file=\"data_gabungan.xlsx\"):\n",
    "    \"\"\"\n",
    "    Membaca file gabungan yang sudah ada (jika ada), menggabungkan dengan artikel baru,\n",
    "    menghilangkan duplikasi berdasarkan URL, dan menyimpan kembali ke file gabungan.\n",
    "    \"\"\"\n",
    "    if os.path.exists(gabungan_file):\n",
    "        df_existing = pd.read_excel(gabungan_file)\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "        \n",
    "    df_new = pd.DataFrame(new_articles)\n",
    "    df_combined = pd.concat([df_new, df_existing], ignore_index=True)\n",
    "    df_combined.drop_duplicates(subset=[\"URL\"], keep=\"first\", inplace=True)\n",
    "    df_combined.to_excel(gabungan_file, index=False)\n",
    "    print(f\"Gabungan file ({gabungan_file}) telah diperbarui dengan {len(df_new)} artikel baru.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detik(file_name=\"data_detik.xlsx\", gabungan_file=\"data_gabungan.xlsx\"):\n",
    "    \"\"\"\n",
    "    Scrapes articles from Detik (page 1) that match disaster-related keywords.\n",
    "    Saves only new articles into an Excel file while keeping old ones, \n",
    "    and also updates the combined file.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.detik.com/tag/bencana-alam/?sortby=time&page=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    keywords = [\n",
    "        r\"\\berupsi\\b\", r\"\\bgunung meletus\\b\", r\"\\btanah longsor\\b\", r\"\\blongsor\\b\",\n",
    "        r\"\\bbanjir\\b\", r\"\\bbanjir bandang\\b\", r\"\\btsunami\\b\",\n",
    "        r\"\\bgempa\\b\", r\"\\bgempa bumi\\b\", r\"\\bbadai\\b\", r\"\\bputing beliung\\b\", r\"\\bangin kencang\\b\",\n",
    "        r\"\\bkekeringan\\b\", r\"\\bkemarau panjang\\b\", r\"\\bhujan es\\b\", r\"\\bgelombang panas\\b\",\n",
    "        r\"\\bcuaca ekstrem\\b\", r\"\\bgelombang ekstrem\\b\", r\"\\bgemuruh laut\\b\",\n",
    "        r\"\\bkebakaran hutan\\b\", r\"\\bkebakaran lahan\\b\", r\"\\bkarhutla\\b\", r\"\\bapi\\b\"\n",
    "    ]\n",
    "    keyword_pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "    \n",
    "    print(\"\\nScraping Detik...\")\n",
    "\n",
    "    # Baca file Detik yang sudah ada untuk cek URL duplikat\n",
    "    if os.path.exists(file_name):\n",
    "        df_existing = pd.read_excel(file_name)\n",
    "        existing_urls = set(df_existing[\"URL\"].tolist())\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "        existing_urls = set()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\")\n",
    "    if not articles:\n",
    "        print(\"No articles found in Detik.\")\n",
    "        return\n",
    "\n",
    "    new_articles = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find(\"a\", href=True)\n",
    "        link = link_tag['href'] if link_tag else None\n",
    "        if not link or \"foto-news\" in link or \"/foto/\" in link or link in existing_urls:\n",
    "            continue\n",
    "\n",
    "        title_tag = article.find(\"h2\", class_=\"title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"No title\"\n",
    "        print(f\"Processing: {title} (Detik)\")\n",
    "\n",
    "        try:\n",
    "            article_response = requests.get(link, headers=headers)\n",
    "            article_response.raise_for_status()\n",
    "            article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "            # Skip jika artikel merupakan \"Video News\"\n",
    "            video_news_tag = article_soup.find(\"h2\", class_=\"detail__subtitle\", string=\"Video News\")\n",
    "            if video_news_tag:\n",
    "                continue\n",
    "\n",
    "            # Hapus elemen-elemen yang tidak diperlukan\n",
    "            for tag in article_soup.find_all(\"div\", class_=\"parallaxindetail scrollpage\"):\n",
    "                tag.decompose()\n",
    "            for tag in article_soup.find_all(\"span\", class_=\"para_caption\", string=\"ADVERTISEMENT\"):\n",
    "                tag.decompose()\n",
    "            for tag in article_soup.find_all(\"p\", class_=\"para_caption\", string=\"SCROLL TO CONTINUE WITH CONTENT\"):\n",
    "                tag.decompose()\n",
    "\n",
    "            # Ambil tanggal\n",
    "            date_tag = article_soup.find(\"div\", class_=\"detail__date\")\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"Unknown Date\"\n",
    "            bulan_mapping = {\n",
    "                \"Jan\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Apr\": \"Apr\", \"Mei\": \"May\",\n",
    "                \"Jun\": \"Jun\", \"Jul\": \"Jul\", \"Agu\": \"Aug\", \"Sep\": \"Sep\", \"Okt\": \"Oct\",\n",
    "                \"Nov\": \"Nov\", \"Des\": \"Dec\"\n",
    "            }\n",
    "            match = re.search(r'(\\d{1,2}) (\\w{3}) (\\d{4})', date)\n",
    "            if match:\n",
    "                day, month, year = match.groups()\n",
    "                month = bulan_mapping.get(month, month)\n",
    "                date_obj = datetime.strptime(f\"{day} {month} {year}\", \"%d %b %Y\")\n",
    "                date = date_obj.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "            # Ambil konten\n",
    "            paragraphs = article_soup.find_all(\"p\")\n",
    "            content = \"\"\n",
    "            for p in paragraphs:\n",
    "                p_text = \"\"\n",
    "                previous_text = \"\"\n",
    "                for element in p.children:\n",
    "                    if element.name == \"a\":\n",
    "                        if previous_text:\n",
    "                            p_text += \" \"\n",
    "                        p_text += element.get_text(strip=True)\n",
    "                        p_text += \" \"\n",
    "                    elif element.name is None:\n",
    "                        text = element.strip()\n",
    "                        if previous_text:\n",
    "                            p_text += \" \" + text\n",
    "                        else:\n",
    "                            p_text += text\n",
    "                    previous_text = p_text.strip()\n",
    "                content += p_text.strip() + \" \"\n",
    "            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching article: {link} (Detik)\")\n",
    "            content = \"Failed to fetch content.\"\n",
    "\n",
    "        if keyword_pattern.search(title) or keyword_pattern.search(content):\n",
    "            new_articles.append({\n",
    "                \"URL\": link,\n",
    "                \"Date\": date,\n",
    "                \"Title\": title,\n",
    "                \"Content\": content\n",
    "            })\n",
    "\n",
    "    if new_articles:\n",
    "        print(f\"Found {len(new_articles)} new articles in Detik. Updating files...\")\n",
    "        # Update file khusus Detik\n",
    "        df_new = pd.DataFrame(new_articles)\n",
    "        df_detik_combined = pd.concat([df_new, df_existing], ignore_index=True)\n",
    "        df_detik_combined.drop_duplicates(subset=[\"URL\"], keep=\"first\", inplace=True)\n",
    "        df_detik_combined.to_excel(file_name, index=False)\n",
    "        print(f\"File {file_name} updated with {len(new_articles)} new articles from Detik.\")\n",
    "        # Update file gabungan\n",
    "        update_gabungan(new_articles, gabungan_file)\n",
    "    else:\n",
    "        print(\"No new articles found from Detik.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_kompas(file_name=\"data_kompas.xlsx\", gabungan_file=\"data_gabungan.xlsx\"):\n",
    "    \"\"\"\n",
    "    Scrapes articles from Kompas that match disaster-related keywords.\n",
    "    Saves only new articles into an Excel file while keeping old ones, \n",
    "    and also updates the combined file.\n",
    "    \"\"\"\n",
    "    url = \"https://www.kompas.com/tag/bencana?page=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    category_patterns = {\n",
    "        \"Banjir\": r\"\\bbanjir\\b|\\bbanjir bandang\\b\",\n",
    "        \"Gempa Bumi\": r\"\\bgempa\\b|\\bgempa bumi\\b\",\n",
    "        \"Tanah Longsor\": r\"\\btanah longsor\\b|\\blongsor\\b\",\n",
    "        \"Gunung Meletus\": r\"\\bgunung meletus\\b|\\berupsi\\b\",\n",
    "        \"Tsunami\": r\"\\btsunami\\b\",\n",
    "        \"Puting Beliung\": r\"\\bputing beliung\\b|\\bbadai\\b\",\n",
    "        \"Kekeringan\": r\"\\bkekeringan\\b|\\bkemarau panjang\\b\",\n",
    "        \"Cuaca Ekstrem\": r\"\\bcuaca ekstrem\\b|\\bhujan es\\b|\\bgelombang panas\\b\",\n",
    "        \"Gelombang Ekstrem\": r\"\\bgelombang ekstrem\\b|\\bgemuruh laut\\b\",\n",
    "        \"Kebakaran Hutan\": r\"\\bkebakaran hutan\\b|\\bapi di hutan\\b|\\bkarhutla\\b\"\n",
    "    }\n",
    "    keyword_pattern = re.compile(\"|\".join(category_patterns.values()), re.IGNORECASE)\n",
    "    \n",
    "    print(\"\\nScraping Kompas...\")\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        df_existing = pd.read_excel(file_name)\n",
    "        existing_urls = set(df_existing[\"URL\"].tolist())\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "        existing_urls = set()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Kompas page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='article__list__title')\n",
    "    if not articles:\n",
    "        print(\"No articles found in Kompas.\")\n",
    "        return\n",
    "\n",
    "    new_articles = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            title = article.find('h3', class_='article__title').get_text(strip=True)\n",
    "            link = article.find('a', class_='article__link')['href']\n",
    "            date = article.find_next_sibling('div', class_='article__list__info') \\\n",
    "                          .find('div', class_='article__date') \\\n",
    "                          .get_text(strip=True)\n",
    "            date = date.split(',')[0]\n",
    "            if link in existing_urls:\n",
    "                continue\n",
    "            print(f\"Processing: {title} (Kompas)\")\n",
    "            article_response = requests.get(link, headers=headers)\n",
    "            if article_response.status_code != 200:\n",
    "                print(f\"Failed to access article: {link}\")\n",
    "                continue\n",
    "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            paragraphs = []\n",
    "            for p in article_soup.find_all('p'):\n",
    "                if p.find('a', class_='inner-link-baca-juga'):\n",
    "                    continue\n",
    "                if p.find_parent('div', class_='footerCopyright'):\n",
    "                    continue\n",
    "                if not p.has_attr('class'):\n",
    "                    text = ''.join([\n",
    "                        elem.strip() if isinstance(elem, str)\n",
    "                        else ' ' + elem.get_text(strip=True) + ' '\n",
    "                        for elem in p.contents\n",
    "                    ])\n",
    "                    paragraphs.append(text)\n",
    "            content = ' '.join(paragraphs)\n",
    "            # Hapus teks sebelum dash pertama (jika bukan di tengah kata)\n",
    "            for i, c in enumerate(content):\n",
    "                if c in ('-', 'â€“'):\n",
    "                    left_is_alnum = (i > 0 and content[i-1].isalnum())\n",
    "                    right_is_alnum = (i < len(content) - 1 and content[i+1].isalnum())\n",
    "                    if left_is_alnum and right_is_alnum:\n",
    "                        continue\n",
    "                    content = content[i+1:].strip()\n",
    "                    break\n",
    "            if keyword_pattern.search(title) or keyword_pattern.search(content):\n",
    "                new_articles.append({\n",
    "                    'URL': link,\n",
    "                    'Date': date,\n",
    "                    'Title': title,\n",
    "                    'Content': content\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "\n",
    "    if new_articles:\n",
    "        print(f\"Found {len(new_articles)} new articles in Kompas. Updating files...\")\n",
    "        df_new = pd.DataFrame(new_articles)\n",
    "        df_kompas_combined = pd.concat([df_new, df_existing], ignore_index=True)\n",
    "        df_kompas_combined.drop_duplicates(subset=[\"URL\"], keep=\"first\", inplace=True)\n",
    "        df_kompas_combined.to_excel(file_name, index=False)\n",
    "        print(f\"File {file_name} updated with {len(new_articles)} new articles from Kompas.\")\n",
    "        update_gabungan(new_articles, gabungan_file)\n",
    "    else:\n",
    "        print(\"No new articles found from Kompas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cnn(file_name=\"data_cnn.xlsx\", gabungan_file=\"data_gabungan.xlsx\"):\n",
    "    \"\"\"\n",
    "    Scrapes CNN Indonesia articles for today's date that match disaster-related keywords.\n",
    "    Saves only new articles into an Excel file while keeping old ones,\n",
    "    and also updates the combined file.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    keywords = [\n",
    "        r\"\\berupsi\\b\", r\"\\bgunung meletus\\b\", r\"\\btanah longsor\\b\", r\"\\blongsor\\b\",\n",
    "        r\"\\bbanjir\\b\", r\"\\bbanjir bandang\\b\", r\"\\btsunami\\b\",\n",
    "        r\"\\bgempa\\b\", r\"\\bgempa bumi\\b\", r\"\\bbadai\\b\", r\"\\bputing beliung\\b\", r\"\\bangin kencang\\b\",\n",
    "        r\"\\bkekeringan\\b\", r\"\\bkemarau panjang\\b\", r\"\\bhujan es\\b\", r\"\\bgelombang panas\\b\",\n",
    "        r\"\\bcuaca ekstrem\\b\", r\"\\bgelombang ekstrem\\b\", r\"\\bgemuruh laut\\b\",\n",
    "        r\"\\bkebakaran hutan\\b\", r\"\\bkebakaran lahan\\b\", r\"\\bkarhutla\\b\", r\"\\bapi\\b\"\n",
    "    ]\n",
    "    keyword_pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "    base_url = \"https://www.cnnindonesia.com/peristiwa/indeks/18\"\n",
    "    print(\"\\nScraping CNN Indonesia...\")\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        df_existing = pd.read_excel(file_name)\n",
    "        existing_urls = set(df_existing[\"URL\"].tolist())\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "        existing_urls = set()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching CNN page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"flex-grow\")\n",
    "    if not articles:\n",
    "        print(\"No articles found in CNN.\")\n",
    "        return\n",
    "\n",
    "    new_articles = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find(\"a\", href=True)\n",
    "        link = link_tag['href'] if link_tag else None\n",
    "        title_tag = article.find(\"h2\", class_=\"text-cnn_black_light dark:text-white mb-2 inline leading-normal text-xl group-hover:text-cnn_red\")\n",
    "        title = title_tag.text.strip() if title_tag else \"No title\"\n",
    "        if \"FOTO\" in title.upper():\n",
    "            print(f\"Skipping article (contains 'FOTO'): {title}\")\n",
    "            continue\n",
    "        if not link or not link.startswith(\"http\") or link in existing_urls:\n",
    "            continue\n",
    "        print(f\"Processing: {title} (CNN)\")\n",
    "        try:\n",
    "            article_response = requests.get(link, headers=headers)\n",
    "            article_response.raise_for_status()\n",
    "            article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "            date_element = article_soup.find(\"div\", class_=\"text-cnn_grey text-sm mb-4\")\n",
    "            date = date_element.text.strip() if date_element else \"No date found\"\n",
    "            bulan_mapping = {\n",
    "                \"Jan\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Apr\": \"Apr\", \"Mei\": \"May\",\n",
    "                \"Jun\": \"Jun\", \"Jul\": \"Jul\", \"Agu\": \"Aug\", \"Sep\": \"Sep\", \"Okt\": \"Oct\",\n",
    "                \"Nov\": \"Nov\", \"Des\": \"Dec\"\n",
    "            }\n",
    "            match = re.search(r'(\\d{1,2}) (\\w{3}) (\\d{4})', date)\n",
    "            if match:\n",
    "                day, month, year = match.groups()\n",
    "                month = bulan_mapping.get(month, month)\n",
    "                date_obj = datetime.strptime(f\"{day} {month} {year}\", \"%d %b %Y\")\n",
    "                date = date_obj.strftime(\"%d/%m/%Y\")\n",
    "            content_element = article_soup.find(\"div\", class_=\"detail-text text-cnn_black text-sm grow min-w-0\")\n",
    "            if content_element:\n",
    "                for ad_div in content_element.find_all(\"div\", class_=\"paradetail\"):\n",
    "                    ad_div.decompose()\n",
    "                content_paragraphs = content_element.find_all(\"p\")\n",
    "                content = \"\"\n",
    "                for p in content_paragraphs:\n",
    "                    p_text = \"\"\n",
    "                    for element in p.children:\n",
    "                        if element.name == \"span\" or element.name == \"a\":\n",
    "                            p_text += \" \" + element.get_text(strip=True) + \" \"\n",
    "                        elif element.name is None:\n",
    "                            p_text += element.strip() + \" \"\n",
    "                    content += p_text.strip() + \" \"\n",
    "                content = re.sub(r'\\s+', ' ', content).strip()\n",
    "            else:\n",
    "                content = \"No content found\"\n",
    "            if keyword_pattern.search(title) or keyword_pattern.search(content):\n",
    "                new_articles.append({\n",
    "                    \"URL\": link,\n",
    "                    \"Date\": date,\n",
    "                    \"Title\": title,\n",
    "                    \"Content\": content\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Skipping article (no matching keywords): {title}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching article {link}: {e}\")\n",
    "\n",
    "    if new_articles:\n",
    "        print(f\"Found {len(new_articles)} new articles in CNN. Updating files...\")\n",
    "        df_new = pd.DataFrame(new_articles)\n",
    "        df_cnn_combined = pd.concat([df_new, df_existing], ignore_index=True)\n",
    "        df_cnn_combined.drop_duplicates(subset=[\"URL\"], keep=\"first\", inplace=True)\n",
    "        df_cnn_combined.to_excel(file_name, index=False)\n",
    "        print(f\"File {file_name} updated with {len(new_articles)} new articles from CNN.\")\n",
    "        update_gabungan(new_articles, gabungan_file)\n",
    "    else:\n",
    "        print(\"No new articles found from CNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrape(interval=120):\n",
    "    \"\"\"\n",
    "    Runs the scraper sequentially (Detik -> Kompas -> CNN) in a loop,\n",
    "    allowing interruption with Ctrl + C.\n",
    "\n",
    "    Args:\n",
    "        interval (int): Time in seconds to wait between each scraping cycle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            print(\"\\n========== Starting New Scraping Cycle ==========\\n\")\n",
    "\n",
    "            # Scrape masing-masing sumber berita\n",
    "            scrape_detik()\n",
    "            scrape_kompas()\n",
    "            scrape_cnn()\n",
    "\n",
    "            print(f\"\\n========== Scraping Completed. Waiting {interval // 60} minutes before next cycle ==========\\n\")\n",
    "            t.sleep(interval)  # Wait before the next cycle\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping stopped by user. Exiting safely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Starting New Scraping Cycle ==========\n",
      "\n",
      "\n",
      "Scraping Detik...\n",
      "Processing: Longsor Terjang Bungbulang Garut, 1 Orang Tertimbun (Detik)\n",
      "Processing: Deretan Foto Fenomena Cuaca Mengerikan, Inikah Kiamat? (Detik)\n",
      "Processing: Penghuni Laut Dalam Ikan Anglerfish Muncul ke Permukaan, Apakah Akan Ada Bencana? (Detik)\n",
      "Processing: 33 Rumah di Tasikmalaya Rusak Akibat Pergeseran Tanah (Detik)\n",
      "Found 1 new articles in Detik. Updating files...\n",
      "File data_detik.xlsx updated with 1 new articles from Detik.\n",
      "Gabungan file (data_gabungan.xlsx) telah diperbarui dengan 1 artikel baru.\n",
      "\n",
      "Scraping Kompas...\n",
      "Processing: Indonesia Rawan Bencana, Ikatan Ahli Dorong Pemerintah Bentuk UU Geologi (Kompas)\n",
      "Processing: Bencana Pergeseran Tanah, 15 Rumah di Sumbawa Segera Direlokasi (Kompas)\n",
      "Processing: Pergerakan Tanah Bikin Warga Cikondang Tasikmalaya Resah jika Hujan Mengguyur Desa... (Kompas)\n",
      "Processing: Pergerakan Tanah di Tasikmalaya Meluas, 44 Rumah Terdampak, Retak-retak (Kompas)\n",
      "Processing: Ancaman Tanah Bergerak di Pasuruan: Mengapa Relokasi Jadi Solusi Terbaik? (Kompas)\n",
      "Processing: Puluhan Rumah di Tasikmalaya Terdampak Pergerakan Tanah, BPBD Lapor Tim Geologi Bandung (Kompas)\n",
      "Processing: Efisiensi Anggaran, BPBD Semarang Harap Dana Logistik Bencana Tak Dipangkas (Kompas)\n",
      "Processing: 40 Honorer BPBD Jember Dibebastugaskan, Bisa Tetap Bekerja tapi Tak Ada Gaji (Kompas)\n",
      "Processing: Bisnis Mikro Bisa Ambruk Kapan Saja, BRI Insurance Tawarkan Perlindungan Murah (Kompas)\n",
      "No new articles found from Kompas.\n",
      "\n",
      "Scraping CNN Indonesia...\n",
      "Processing: Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang (CNN)\n",
      "Skipping article (no matching keywords): Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang\n",
      "Processing: Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar (CNN)\n",
      "Skipping article (no matching keywords): Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar\n",
      "Processing: TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara (CNN)\n",
      "Skipping article (no matching keywords): TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara\n",
      "Processing: Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar (CNN)\n",
      "Skipping article (no matching keywords): Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar\n",
      "Processing: Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah (CNN)\n",
      "Skipping article (no matching keywords): Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah\n",
      "Skipping article (contains 'FOTO'): FOTO: Banjir Rendam Puluhan Rumah di Jambi\n",
      "Processing: Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam (CNN)\n",
      "Skipping article (no matching keywords): Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam\n",
      "Processing: Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi (CNN)\n",
      "Skipping article (no matching keywords): Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi\n",
      "No new articles found from CNN.\n",
      "\n",
      "========== Scraping Completed. Waiting 0 minutes before next cycle ==========\n",
      "\n",
      "\n",
      "========== Starting New Scraping Cycle ==========\n",
      "\n",
      "\n",
      "Scraping Detik...\n",
      "Processing: Deretan Foto Fenomena Cuaca Mengerikan, Inikah Kiamat? (Detik)\n",
      "Processing: Penghuni Laut Dalam Ikan Anglerfish Muncul ke Permukaan, Apakah Akan Ada Bencana? (Detik)\n",
      "Processing: 33 Rumah di Tasikmalaya Rusak Akibat Pergeseran Tanah (Detik)\n",
      "No new articles found from Detik.\n",
      "\n",
      "Scraping Kompas...\n",
      "Processing: Indonesia Rawan Bencana, Ikatan Ahli Dorong Pemerintah Bentuk UU Geologi (Kompas)\n",
      "Processing: Bencana Pergeseran Tanah, 15 Rumah di Sumbawa Segera Direlokasi (Kompas)\n",
      "Processing: Pergerakan Tanah Bikin Warga Cikondang Tasikmalaya Resah jika Hujan Mengguyur Desa... (Kompas)\n",
      "\n",
      "Scraping stopped by user. Exiting safely.\n"
     ]
    }
   ],
   "source": [
    "run_scrape(interval=10)  # Tunggu 5 menit antar siklus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_jenis = joblib.load('./tfidf_vectorizer_jenis.pkl')\n",
    "model_jenis = joblib.load('./model_xgboost_jenis.pkl',)\n",
    "vectorizer_dampak = joblib.load('./tfidf_vectorizer_dampak.pkl')\n",
    "model_dampak = joblib.load('./model_xgboost_dampak.pkl',)\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model ner')\n",
    "model = AutoModelForTokenClassification.from_pretrained('./model ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'Banjir',\n",
    "    1: 'Bencana Hidrometerologi Ekstrem',\n",
    "    2: 'Gempa Bumi',\n",
    "    3: 'Gunung Meletus',\n",
    "    4: 'Puting Beliung',\n",
    "    5: 'Tanah Longsor',\n",
    "    6: 'Tsunami'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membersihkan teks pada konten\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan teks pada kolom Content:\n",
    "    - Mengubah ke huruf kecil\n",
    "    - Menghapus angka\n",
    "    - Menghapus stopwords\n",
    "    - Menghapus spasi berlebih\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_decimal_points(text):\n",
    "    \"\"\"\n",
    "    Mengganti titik dalam angka desimal dengan placeholder <DECIMAL>.\n",
    "    Contoh: '20.46' menjadi '20<DECIMAL>46'\n",
    "    \"\"\"\n",
    "    text = re.sub(r'(\\d)\\.(\\d)', r'\\1<DECIMAL>\\2', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_quoted_dots(text):\n",
    "    \"\"\"\n",
    "    Mengganti titik dalam kalimat kutipan dengan placeholder <QUOTE_DOT>.\n",
    "    Contoh: '\"Informasi ini penting.\"' menjadi '\"Informasi ini penting<QUOTE_DOT>\"'\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\.(?=\\\")', r'<QUOTE_DOT>', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_special_cases(text):\n",
    "    \"\"\"\n",
    "    Menangani kasus khusus lainnya, seperti titik setelah singkatan dalam tanda kurung.\n",
    "    Contoh: 'Senin (1/1).' menjadi 'Senin (1/1)'\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\((\\d+)/(\\d+)\\)\\.', r'(\\1/\\2)', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Melakukan semua langkah preprocessing pada teks:\n",
    "    - Mengganti titik dalam angka desimal dengan placeholder <DECIMAL>.\n",
    "    - Mengganti titik dalam kalimat kutipan dengan placeholder <QUOTE_DOT>.\n",
    "    - Menangani kasus khusus lainnya seperti titik setelah singkatan dalam tanda kurung.\n",
    "    \"\"\"\n",
    "    text = preprocess_decimal_points(text)\n",
    "    text = preprocess_quoted_dots(text)\n",
    "    text = preprocess_special_cases(text)\n",
    "    return text\n",
    "\n",
    "def postprocess_decimal_points(sentences):\n",
    "    \"\"\"\n",
    "    Mengembalikan placeholder <DECIMAL> dan <QUOTE_DOT> menjadi titik.\n",
    "    \"\"\"\n",
    "    if isinstance(sentences, list):\n",
    "        sentences = [sentence.replace('<DECIMAL>', '.') for sentence in sentences]\n",
    "        sentences = [sentence.replace('<QUOTE_DOT>', '.') for sentence in sentences]\n",
    "    elif isinstance(sentences, str):\n",
    "        sentences = sentences.replace('<DECIMAL>', '.').replace('<QUOTE_DOT>', '.')\n",
    "    else:\n",
    "        raise ValueError(\"Input harus berupa string atau list of strings.\")\n",
    "    return sentences\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize kalimat menggunakan NLTK, menangani titik dalam angka desimal dan kalimat kutipan:\n",
    "    - Melakukan preprocessing untuk menangani titik dalam angka desimal dan tanda baca dalam kutipan.\n",
    "    - Menggunakan NLTK untuk memisahkan teks menjadi kalimat.\n",
    "    - Mengembalikan titik dalam angka desimal dan tanda baca dalam kutipan setelah tokenisasi.\n",
    "    \"\"\"\n",
    "    text = preprocess_text(text)  # Preprocessing\n",
    "    sentences = sent_tokenize(text)  # Tokenisasi menggunakan NLTK\n",
    "    sentences = postprocess_decimal_points(sentences)  # Postprocessing\n",
    "    return sentences\n",
    "\n",
    "def ner_with_chunking_and_cleaning(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Fungsi untuk melakukan NER pada teks panjang dengan pembagian chunk dan penanganan token hashtag (##).\n",
    "\n",
    "    Args:\n",
    "    - text (str): Teks yang akan diproses.\n",
    "    - max_length (int): Batas panjang token per chunk (default: 512).\n",
    "\n",
    "    Returns:\n",
    "    - List[Dict]: Hasil NER yang sudah dibersihkan.\n",
    "    \"\"\"\n",
    "    # Fungsi untuk membagi teks menjadi chunk\n",
    "    def split_text_into_chunks(text, max_length):\n",
    "        tokens = tokenizer.encode(text, truncation=False)  # Encode tanpa truncation\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), max_length - 2):  # Memberi ruang untuk [CLS] dan [SEP]\n",
    "            chunk = tokens[i:i + max_length - 2]\n",
    "            # Menambahkan [CLS] dan [SEP] token\n",
    "            chunk = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    # Fungsi untuk membersihkan hasil NER (menghapus tanda ##)\n",
    "    def clean_ner_results(results):\n",
    "        cleaned_results = []\n",
    "        for result in results:\n",
    "            word = result['word']\n",
    "            # Menggabungkan token dengan ##\n",
    "            if word.startswith(\"##\"):\n",
    "                if cleaned_results:\n",
    "                    cleaned_results[-1]['word'] += word[2:]  # Menggabungkan ke token sebelumnya\n",
    "            else:\n",
    "                cleaned_results.append(result)\n",
    "        return cleaned_results\n",
    "\n",
    "    # Membuat pipeline NER\n",
    "    pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "    # Membagi teks menjadi chunk\n",
    "    chunks = split_text_into_chunks(text, max_length)\n",
    "\n",
    "    # Memproses setiap chunk\n",
    "    all_results = []\n",
    "    for chunk in chunks:\n",
    "        decoded_chunk = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        ner_results = pipe(decoded_chunk)\n",
    "        cleaned_chunk_results = clean_ner_results(ner_results)  # Membersihkan hashtag\n",
    "        all_results.extend(cleaned_chunk_results)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def insert_ner_results_to_df(df, results):\n",
    "    # Variabel penyimpanan\n",
    "    location = []\n",
    "    date = None\n",
    "    time = None\n",
    "\n",
    "    # Proses entitas\n",
    "    gpe_sequence = []  # Untuk menyimpan GPE secara berurutan\n",
    "    gpe_found = False\n",
    "    for result in results:\n",
    "        entity_group = result.get(\"entity_group\", None)\n",
    "        word = result.get(\"word\", None)\n",
    "\n",
    "        # Ambil lokasi (GPE) berurutan maksimal 4\n",
    "        if entity_group == \"GPE\":\n",
    "            if not gpe_found:\n",
    "                gpe_found = True\n",
    "            if len(gpe_sequence) < 4:  # Maksimal 4 lokasi\n",
    "                gpe_sequence.append(word)\n",
    "            else:\n",
    "                break  # Stop setelah 4 GPE berturut-turut\n",
    "        elif gpe_found:  # Reset jika bukan GPE berikutnya\n",
    "            break\n",
    "\n",
    "    # Kapitalisasi huruf awal setiap kata di lokasi\n",
    "    location = \", \".join([loc.title() for loc in gpe_sequence])\n",
    "\n",
    "    # Proses tanggal (DAT) yang muncul paling awal\n",
    "    for result in results:\n",
    "        if result.get(\"entity_group\") == \"DAT\" and date is None:\n",
    "            word = result.get(\"word\", None)\n",
    "            try:\n",
    "                # Hapus nama hari dan teks tambahan dengan regex\n",
    "                match = re.search(r\"(\\d{1,2}\\s*/\\s*\\d{1,2}\\s*/\\s*\\d{4})\", word)  # Format dengan tahun\n",
    "                if match:\n",
    "                    cleaned_word = match.group(1).replace(\" \", \"\").replace(\"/\", \"/\")\n",
    "                    # Format tanggal menjadi seperti \"27 November 2024\" tanpa nama hari\n",
    "                    date_obj = datetime.strptime(cleaned_word, \"%d/%m/%Y\")\n",
    "                    date = date_obj.strftime(\"%d %B %Y\")\n",
    "                else:\n",
    "                    # Format ketika tanggal tanpa tahun seperti (22/5)\n",
    "                    match = re.search(r\"(\\d{1,2}\\s*/\\s*\\d{1,2})\", word)\n",
    "                    if match:\n",
    "                        cleaned_word = match.group(1).replace(\" \", \"\").replace(\"/\", \"/\")\n",
    "                        current_year = datetime.now().year  # Gunakan tahun saat ini\n",
    "                        cleaned_word += f\"/{current_year}\"\n",
    "                        date_obj = datetime.strptime(cleaned_word, \"%d/%m/%Y\")\n",
    "                        date = date_obj.strftime(\"%d %B\")\n",
    "            except ValueError:\n",
    "                date = word  # Jika format tidak sesuai, simpan apa adanya\n",
    "\n",
    "    # Proses waktu (TIM) yang muncul paling awal\n",
    "    for result in results:\n",
    "        if result.get(\"entity_group\") == \"TIM\" and time is None:\n",
    "            word = result.get(\"word\", None)\n",
    "            # Normalisasi waktu dengan regex\n",
    "            match = re.search(r\"(\\d{1,2})\\.\\s*(\\d{2})\\s*(wib|wita|wit)?\", word, re.IGNORECASE)\n",
    "            if match:\n",
    "                hours = match.group(1)\n",
    "                minutes = match.group(2)\n",
    "                timezone = match.group(3).upper() if match.group(3) else \"\"\n",
    "                time = f\"{hours}.{minutes} {timezone}\".strip()\n",
    "            else:\n",
    "                time = word  # Simpan apa adanya jika tidak sesuai pola\n",
    "\n",
    "    # Jika waktu tidak ditemukan, masukkan keterangan default\n",
    "    if time is None:\n",
    "        time = \"Tidak ada dalam artikel\"\n",
    "\n",
    "    # Masukkan hasil ke DataFrame\n",
    "    df.loc[len(df)] = [location, date, time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediksi Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fungsi prediksi jenis bencana\n",
    "def predict_jenis(konten):\n",
    "    jenis_cleaned = clean_text(konten)\n",
    "    jenis_vectorized = vectorizer_jenis.transform([jenis_cleaned])\n",
    "    jenis_prediction = model_jenis.predict(jenis_vectorized)\n",
    "    # Konversi prediksi angka ke label string\n",
    "    jenis_prediction = [label_mapping[pred] for pred in jenis_prediction]\n",
    "    return jenis_prediction[0]  # Mengembalikan prediksi pertama\n",
    "\n",
    "# 2. Fungsi NER dengan chunking dan cleaning\n",
    "def ner_with_chunking_and_cleaning(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Melakukan NER pada teks panjang dengan pembagian chunk dan penanganan token hashtag (##).\n",
    "    \"\"\"\n",
    "    def split_text_into_chunks(text, max_length):\n",
    "        tokens = tokenizer.encode(text, truncation=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), max_length - 2):  # Ruang untuk [CLS] dan [SEP]\n",
    "            chunk = tokens[i:i + max_length - 2]\n",
    "            chunk = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def clean_ner_results(results):\n",
    "        cleaned_results = []\n",
    "        for result in results:\n",
    "            word = result['word']\n",
    "            if word.startswith(\"##\") and cleaned_results:\n",
    "                cleaned_results[-1]['word'] += word[2:]\n",
    "            else:\n",
    "                cleaned_results.append(result)\n",
    "        return cleaned_results\n",
    "\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "    chunks = split_text_into_chunks(text, max_length)\n",
    "    all_results = []\n",
    "    for chunk in chunks:\n",
    "        decoded_chunk = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        ner_results = ner_pipe(decoded_chunk)\n",
    "        cleaned_chunk_results = clean_ner_results(ner_results)\n",
    "        all_results.extend(cleaned_chunk_results)\n",
    "    return all_results\n",
    "\n",
    "# Fungsi untuk mengekstrak entitas NER (lokasi, tanggal, waktu)\n",
    "def extract_ner_entities(results):\n",
    "    location = \"\"\n",
    "    date = None\n",
    "    time = None\n",
    "\n",
    "    # Ekstraksi lokasi (GPE) secara berurutan maksimal 4 lokasi\n",
    "    gpe_sequence = []\n",
    "    gpe_found = False\n",
    "    for result in results:\n",
    "        entity_group = result.get(\"entity_group\")\n",
    "        word = result.get(\"word\")\n",
    "        if entity_group == \"GPE\":\n",
    "            if not gpe_found:\n",
    "                gpe_found = True\n",
    "            if len(gpe_sequence) < 4:\n",
    "                gpe_sequence.append(word)\n",
    "            else:\n",
    "                break\n",
    "        elif gpe_found:\n",
    "            break\n",
    "    location = \", \".join([loc.title() for loc in gpe_sequence])\n",
    "    \n",
    "    # Ekstraksi tanggal (DAT), ambil yang pertama kali muncul\n",
    "    for result in results:\n",
    "        if result.get(\"entity_group\") == \"DAT\" and date is None:\n",
    "            word = result.get(\"word\")\n",
    "            try:\n",
    "                match = re.search(r\"(\\d{1,2}\\s*/\\s*\\d{1,2}\\s*/\\s*\\d{4})\", word)\n",
    "                if match:\n",
    "                    cleaned_word = match.group(1).replace(\" \", \"\")\n",
    "                    date_obj = datetime.strptime(cleaned_word, \"%d/%m/%Y\")\n",
    "                    date = date_obj.strftime(\"%d %B %Y\")\n",
    "                else:\n",
    "                    match = re.search(r\"(\\d{1,2}\\s*/\\s*\\d{1,2})\", word)\n",
    "                    if match:\n",
    "                        cleaned_word = match.group(1).replace(\" \", \"\")\n",
    "                        current_year = datetime.now().year\n",
    "                        cleaned_word += f\"/{current_year}\"\n",
    "                        date_obj = datetime.strptime(cleaned_word, \"%d/%m/%Y\")\n",
    "                        date = date_obj.strftime(\"%d %B\")\n",
    "            except ValueError:\n",
    "                date = word\n",
    "\n",
    "    # Ekstraksi waktu (TIM), ambil yang pertama kali muncul\n",
    "    for result in results:\n",
    "        if result.get(\"entity_group\") == \"TIM\" and time is None:\n",
    "            word = result.get(\"word\")\n",
    "            match = re.search(r\"(\\d{1,2})\\.\\s*(\\d{2})\\s*(wib|wita|wit)?\", word, re.IGNORECASE)\n",
    "            if match:\n",
    "                hours = match.group(1)\n",
    "                minutes = match.group(2)\n",
    "                timezone = match.group(3).upper() if match.group(3) else \"\"\n",
    "                time = f\"{hours}.{minutes} {timezone}\".strip()\n",
    "            else:\n",
    "                time = word\n",
    "    if time is None:\n",
    "        time = \"Tidak ada dalam artikel\"\n",
    "    return location, date, time\n",
    "\n",
    "# 3. Fungsi prediksi kalimat dampak bencana\n",
    "def predict_impact(konten):\n",
    "    kalimat_list = sentence_tokenize(konten)\n",
    "    dampak_vectorized = vectorizer_dampak.transform(kalimat_list)\n",
    "    dampak_pred = model_dampak.predict(dampak_vectorized)\n",
    "    kalimat_berdampak = [kalimat for kalimat, label in zip(kalimat_list, dampak_pred) if label == 1]\n",
    "    return \" \".join(kalimat_berdampak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Baca file data_prediksi.xlsx jika ada, jika tidak buat DataFrame baru\n",
    "if os.path.exists(\"data_prediksi.xlsx\"):\n",
    "    df_prediksi = pd.read_excel(\"data_prediksi.xlsx\")\n",
    "else:\n",
    "    df_prediksi = pd.DataFrame(columns=[\"URL\", \"Type\", \"Location\", \"Date\", \"Time\", \"Impact\"])\n",
    "\n",
    "# Baca file data_gabungan.xlsx jika ada, jika tidak buat DataFrame baru\n",
    "if os.path.exists(\"data_gabungan.xlsx\"):\n",
    "    df_gabungan = pd.read_excel(\"data_gabungan.xlsx\")\n",
    "else:\n",
    "    df_gabungan = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "\n",
    "# Iterasi untuk tiap baris di df_gabungan\n",
    "for index, row in df_gabungan.iterrows():\n",
    "    url = row.get(\"URL\", None)\n",
    "    \n",
    "    # Cek apakah URL sudah pernah diprediksi sebelumnya\n",
    "    if url is not None and url in df_prediksi['URL'].values:\n",
    "        continue  # Lewati baris jika URL sudah ada\n",
    "    \n",
    "    konten = row['Content']\n",
    "    \n",
    "    # 1. Prediksi jenis bencana\n",
    "    pred_type = predict_jenis(konten)\n",
    "    \n",
    "    # 2. Ekstraksi entitas NER: lokasi, tanggal, waktu\n",
    "    ner_results = ner_with_chunking_and_cleaning(konten)\n",
    "    location, date, time = extract_ner_entities(ner_results)\n",
    "    \n",
    "    # 3. Prediksi kalimat dampak bencana\n",
    "    impact_text = predict_impact(konten)\n",
    "    \n",
    "    # Memasukkan hasil ke df_prediksi\n",
    "    new_row = {\n",
    "        \"URL\": url,\n",
    "        \"Type\": pred_type,\n",
    "        \"Location\": location,\n",
    "        \"Date\": date,\n",
    "        \"Time\": time,\n",
    "        \"Impact\": impact_text\n",
    "    }\n",
    "    df_prediksi = df_prediksi.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Simpan pembaruan ke file Excel\n",
    "    df_prediksi.to_excel(\"data_prediksi.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Type</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7791405/k...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Kecamatan Cijeungjing, Kabupaten Ciamis</td>\n",
       "      <td>22 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Sebanyak 7 bangunan terdiri dari rumah warga d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7786842/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Bulukumba, Sulawesi Selatan, Sulsel</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>14.30 WITA</td>\n",
       "      <td>Sebanyak 14 unit rumah mengalami kerusakan aki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.detik.com/edu/detikpedia/d-7786373...</td>\n",
       "      <td>Gempa Bumi</td>\n",
       "      <td>Jepang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.detik.com/jatim/berita/d-7782941/a...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Sidoarjo</td>\n",
       "      <td>17 February 2025</td>\n",
       "      <td>14.30 WIB</td>\n",
       "      <td>Akibatnya, puluhan rumah warga mengalami kerus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7780631/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Banggai, Sulawesi Tengah, Sulteng</td>\n",
       "      <td>16 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Asrama Polsek Balantak di Kabupaten Banggai, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7780166/h...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kecamatan Sukaraja, Kabupaten Tasikmalaya, Jaw...</td>\n",
       "      <td>15 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Di Desa Leuwibudah dua rumah tertimpa pohon se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.detik.com/bali/nusra/d-7774720/fen...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Desa Tangkampulit, Kecamatan Batulanteh, Kabup...</td>\n",
       "      <td>12 February 2025</td>\n",
       "      <td>11.00 WITA</td>\n",
       "      <td>Sebanyak 15 rumah warga terdampak fenomena tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7772781/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Bulukumba, Sulawesi Selatan, Sulsel</td>\n",
       "      <td>11 February 2025</td>\n",
       "      <td>14.30 WITA</td>\n",
       "      <td>Sebanyak 15 unit rumah dan 1 masjid di Kabupat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/20/171...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Flores Timur, Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>21 December 2024</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://surabaya.kompas.com/read/2025/02/20/110...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Kabupaten Lumajang, Jawa Timur</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>00.00 WIB</td>\n",
       "      <td>Sebanyak empat kali erupsi terpantau jelas sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://denpasar.kompas.com/read/2025/02/19/160...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Banjar Dinas Suci, Desa Tejakula, Kecamatan Te...</td>\n",
       "      <td>05 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Krisis tersebut akibat kerusakan pipa induk im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/18/201...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Timur, Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>12 February 2025</td>\n",
       "      <td>03.00 WITA</td>\n",
       "      <td>Penjabat Bupati Flores Timur, Nusa Tenggara Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/14/170...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>14 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Kejaksaan Negeri Lembata , Nusa Tenggara Timur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://money.kompas.com/read/2025/02/14/073000...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Desa Sungai Itik, Kecamatan Sadu, Kabupaten Ta...</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Terbaru, PetroChina menyalurkan bantuan bagi k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Desa Sisundung, Kecamatan Angkola Barat, Kabup...</td>\n",
       "      <td>22 February</td>\n",
       "      <td>18.00 WIB</td>\n",
       "      <td>Tanah longsor yang terjadi di Desa Sisundung, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Kota Bandar Lampung</td>\n",
       "      <td>23 February</td>\n",
       "      <td>00.51 WIB</td>\n",
       "      <td>Kementerian Sosial (Kemensos) mengirimkan bant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7792142/l...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Bungbulang, Garut</td>\n",
       "      <td>23 February 2025</td>\n",
       "      <td>18.30 WIB</td>\n",
       "      <td>Hingga Minggu malam pukul 20.46 WIB ini, korba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250224...</td>\n",
       "      <td>Gempa Bumi</td>\n",
       "      <td>Waingapu, Sumba Timur, Ntt</td>\n",
       "      <td>24 February</td>\n",
       "      <td>02.21 WIB</td>\n",
       "      <td>[Gambas:Twitter] Hingga saat ini belum diketah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL            Type  \\\n",
       "0   https://www.detik.com/jabar/berita/d-7791405/k...          Banjir   \n",
       "1   https://www.detik.com/sulsel/berita/d-7786842/...  Puting Beliung   \n",
       "2   https://www.detik.com/edu/detikpedia/d-7786373...      Gempa Bumi   \n",
       "3   https://www.detik.com/jatim/berita/d-7782941/a...  Puting Beliung   \n",
       "4   https://www.detik.com/sulsel/berita/d-7780631/...  Puting Beliung   \n",
       "5   https://www.detik.com/jabar/berita/d-7780166/h...  Puting Beliung   \n",
       "6   https://www.detik.com/bali/nusra/d-7774720/fen...   Tanah Longsor   \n",
       "7   https://www.detik.com/sulsel/berita/d-7772781/...  Puting Beliung   \n",
       "8   http://regional.kompas.com/read/2025/02/20/171...  Gunung Meletus   \n",
       "9   http://surabaya.kompas.com/read/2025/02/20/110...  Gunung Meletus   \n",
       "10  http://denpasar.kompas.com/read/2025/02/19/160...          Banjir   \n",
       "11  http://regional.kompas.com/read/2025/02/18/201...  Gunung Meletus   \n",
       "12  http://regional.kompas.com/read/2025/02/14/170...          Banjir   \n",
       "13  http://money.kompas.com/read/2025/02/14/073000...          Banjir   \n",
       "14  https://www.cnnindonesia.com/nasional/20250223...   Tanah Longsor   \n",
       "15  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "16  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "17  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "18  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "19  https://www.detik.com/jabar/berita/d-7792142/l...   Tanah Longsor   \n",
       "20  https://www.cnnindonesia.com/nasional/20250224...      Gempa Bumi   \n",
       "\n",
       "                                             Location              Date  \\\n",
       "0             Kecamatan Cijeungjing, Kabupaten Ciamis  22 February 2025   \n",
       "1       Kabupaten Bulukumba, Sulawesi Selatan, Sulsel  20 February 2025   \n",
       "2                                              Jepang               NaN   \n",
       "3                                  Kabupaten Sidoarjo  17 February 2025   \n",
       "4         Kabupaten Banggai, Sulawesi Tengah, Sulteng  16 February 2025   \n",
       "5   Kecamatan Sukaraja, Kabupaten Tasikmalaya, Jaw...       15 February   \n",
       "6   Desa Tangkampulit, Kecamatan Batulanteh, Kabup...  12 February 2025   \n",
       "7       Kabupaten Bulukumba, Sulawesi Selatan, Sulsel  11 February 2025   \n",
       "8              Flores Timur, Nusa Tenggara Timur, Ntt  21 December 2024   \n",
       "9                      Kabupaten Lumajang, Jawa Timur  20 February 2025   \n",
       "10  Banjar Dinas Suci, Desa Tejakula, Kecamatan Te...  05 February 2025   \n",
       "11                    Timur, Nusa Tenggara Timur, Ntt  12 February 2025   \n",
       "12                           Nusa Tenggara Timur, Ntt  14 February 2025   \n",
       "13  Desa Sungai Itik, Kecamatan Sadu, Kabupaten Ta...  13 February 2025   \n",
       "14  Desa Sisundung, Kecamatan Angkola Barat, Kabup...       22 February   \n",
       "15                                Kota Bandar Lampung       23 February   \n",
       "16                                      Jakarta Utara       23 February   \n",
       "17                                      Jakarta Utara       23 February   \n",
       "18                                      Jakarta Utara       23 February   \n",
       "19                                  Bungbulang, Garut  23 February 2025   \n",
       "20                         Waingapu, Sumba Timur, Ntt       24 February   \n",
       "\n",
       "                       Time                                             Impact  \n",
       "0   Tidak ada dalam artikel  Sebanyak 7 bangunan terdiri dari rumah warga d...  \n",
       "1                14.30 WITA  Sebanyak 14 unit rumah mengalami kerusakan aki...  \n",
       "2   Tidak ada dalam artikel                                                NaN  \n",
       "3                 14.30 WIB  Akibatnya, puluhan rumah warga mengalami kerus...  \n",
       "4   Tidak ada dalam artikel  Asrama Polsek Balantak di Kabupaten Banggai, S...  \n",
       "5   Tidak ada dalam artikel  Di Desa Leuwibudah dua rumah tertimpa pohon se...  \n",
       "6                11.00 WITA  Sebanyak 15 rumah warga terdampak fenomena tan...  \n",
       "7                14.30 WITA  Sebanyak 15 unit rumah dan 1 masjid di Kabupat...  \n",
       "8   Tidak ada dalam artikel                                                NaN  \n",
       "9                 00.00 WIB  Sebanyak empat kali erupsi terpantau jelas sec...  \n",
       "10  Tidak ada dalam artikel  Krisis tersebut akibat kerusakan pipa induk im...  \n",
       "11               03.00 WITA  Penjabat Bupati Flores Timur, Nusa Tenggara Ti...  \n",
       "12  Tidak ada dalam artikel  Kejaksaan Negeri Lembata , Nusa Tenggara Timur...  \n",
       "13  Tidak ada dalam artikel  Terbaru, PetroChina menyalurkan bantuan bagi k...  \n",
       "14                18.00 WIB  Tanah longsor yang terjadi di Desa Sisundung, ...  \n",
       "15                00.51 WIB  Kementerian Sosial (Kemensos) mengirimkan bant...  \n",
       "16  Tidak ada dalam artikel                                                NaN  \n",
       "17  Tidak ada dalam artikel                                                NaN  \n",
       "18  Tidak ada dalam artikel                                                NaN  \n",
       "19                18.30 WIB  Hingga Minggu malam pukul 20.46 WIB ini, korba...  \n",
       "20                02.21 WIB  [Gambas:Twitter] Hingga saat ini belum diketah...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrasi Scrape dan Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scrape_predict(interval=120):\n",
    "    \"\"\"\n",
    "    Menjalankan proses scraping secara berurutan (Detik -> Kompas -> CNN) dalam satu siklus,\n",
    "    kemudian melakukan prediksi pada artikel baru yang ada di data_gabungan.xlsx,\n",
    "    dan menyimpan hasil prediksi ke data_prediksi.xlsx.\n",
    "    \n",
    "    Args:\n",
    "        interval (int): Waktu tunggu (dalam detik) antara siklus.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            print(\"\\n========== Starting New Scraping Cycle ==========\\n\")\n",
    "            \n",
    "            # Jalankan scraping dari masing-masing sumber berita\n",
    "            scrape_detik()   # Fungsi ini meng-update file data_detik.xlsx dan data_gabungan.xlsx\n",
    "            scrape_kompas()  # Fungsi ini meng-update file data_kompas.xlsx dan data_gabungan.xlsx\n",
    "            scrape_cnn()     # Fungsi ini meng-update file data_cnn.xlsx dan data_gabungan.xlsx\n",
    "            \n",
    "            print(\"\\n========== Scraping Completed. Starting Prediction Process ==========\\n\")\n",
    "            \n",
    "            # Muat ulang file data_gabungan.xlsx (mungkin telah terupdate dengan artikel baru)\n",
    "            if os.path.exists(\"data_gabungan.xlsx\"):\n",
    "                df_gabungan = pd.read_excel(\"data_gabungan.xlsx\")\n",
    "            else:\n",
    "                df_gabungan = pd.DataFrame(columns=[\"URL\", \"Date\", \"Title\", \"Content\"])\n",
    "            \n",
    "            # Baca atau inisialisasi file data_prediksi.xlsx\n",
    "            if os.path.exists(\"data_prediksi.xlsx\"):\n",
    "                df_prediksi = pd.read_excel(\"data_prediksi.xlsx\")\n",
    "            else:\n",
    "                df_prediksi = pd.DataFrame(columns=[\"URL\", \"Type\", \"Location\", \"Date\", \"Time\", \"Impact\"])\n",
    "            \n",
    "            # Iterasi untuk tiap baris di df_gabungan dan lakukan prediksi jika URL belum ada di df_prediksi\n",
    "            for index, row in df_gabungan.iterrows():\n",
    "                url = row.get(\"URL\", None)\n",
    "                if url is not None and url in df_prediksi['URL'].values:\n",
    "                    continue  # Lewati jika URL sudah diprediksi\n",
    "                \n",
    "                konten = row['Content']\n",
    "                \n",
    "                # 1. Prediksi jenis bencana\n",
    "                pred_type = predict_jenis(konten)\n",
    "                \n",
    "                # 2. Ekstraksi entitas NER: lokasi, tanggal, waktu\n",
    "                ner_results = ner_with_chunking_and_cleaning(konten)\n",
    "                location, date, time_extracted = extract_ner_entities(ner_results)\n",
    "                \n",
    "                # 3. Prediksi kalimat dampak bencana\n",
    "                impact_text = predict_impact(konten)\n",
    "                \n",
    "                # Masukkan hasil prediksi ke df_prediksi\n",
    "                new_row = {\n",
    "                    \"URL\": url,\n",
    "                    \"Type\": pred_type,\n",
    "                    \"Location\": location,\n",
    "                    \"Date\": date,\n",
    "                    \"Time\": time_extracted,\n",
    "                    \"Impact\": impact_text\n",
    "                }\n",
    "                df_prediksi = df_prediksi.append(new_row, ignore_index=True)\n",
    "                \n",
    "                # Simpan pembaruan ke file Excel\n",
    "                df_prediksi.to_excel(\"data_prediksi.xlsx\", index=False)\n",
    "            \n",
    "            print(\"Prediction process completed. Data prediksi telah diperbarui.\")\n",
    "            print(f\"\\n========== Cycle Completed. Waiting {interval // 60} minutes before next cycle ==========\\n\")\n",
    "            t.sleep(interval)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping and prediction stopped by user. Exiting safely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Starting New Scraping Cycle ==========\n",
      "\n",
      "\n",
      "Scraping Detik...\n",
      "Processing: Longsor Terjang Bungbulang Garut, 1 Orang Tertimbun (Detik)\n",
      "Processing: Deretan Foto Fenomena Cuaca Mengerikan, Inikah Kiamat? (Detik)\n",
      "Processing: Penghuni Laut Dalam Ikan Anglerfish Muncul ke Permukaan, Apakah Akan Ada Bencana? (Detik)\n",
      "Processing: 33 Rumah di Tasikmalaya Rusak Akibat Pergeseran Tanah (Detik)\n",
      "Found 1 new articles in Detik. Updating files...\n",
      "File data_detik.xlsx updated with 1 new articles from Detik.\n",
      "Gabungan file (data_gabungan.xlsx) telah diperbarui dengan 1 artikel baru.\n",
      "\n",
      "Scraping Kompas...\n",
      "Processing: Indonesia Rawan Bencana, Ikatan Ahli Dorong Pemerintah Bentuk UU Geologi (Kompas)\n",
      "Processing: Bencana Pergeseran Tanah, 15 Rumah di Sumbawa Segera Direlokasi (Kompas)\n",
      "Processing: Pergerakan Tanah Bikin Warga Cikondang Tasikmalaya Resah jika Hujan Mengguyur Desa... (Kompas)\n",
      "Processing: Pergerakan Tanah di Tasikmalaya Meluas, 44 Rumah Terdampak, Retak-retak (Kompas)\n",
      "Processing: Ancaman Tanah Bergerak di Pasuruan: Mengapa Relokasi Jadi Solusi Terbaik? (Kompas)\n",
      "Processing: Puluhan Rumah di Tasikmalaya Terdampak Pergerakan Tanah, BPBD Lapor Tim Geologi Bandung (Kompas)\n",
      "Processing: Efisiensi Anggaran, BPBD Semarang Harap Dana Logistik Bencana Tak Dipangkas (Kompas)\n",
      "Processing: 40 Honorer BPBD Jember Dibebastugaskan, Bisa Tetap Bekerja tapi Tak Ada Gaji (Kompas)\n",
      "Processing: PetroChina Bantu Penanggulangan Bencana di Sekitar Wilayah Kerja Jabung (Kompas)\n",
      "Processing: Bisnis Mikro Bisa Ambruk Kapan Saja, BRI Insurance Tawarkan Perlindungan Murah (Kompas)\n",
      "Found 1 new articles in Kompas. Updating files...\n",
      "File data_kompas.xlsx updated with 1 new articles from Kompas.\n",
      "Gabungan file (data_gabungan.xlsx) telah diperbarui dengan 1 artikel baru.\n",
      "\n",
      "Scraping CNN Indonesia...\n",
      "Processing: Gempa Magnitudo 5,3 Guncang Waingapu NTT, Bajawa-Labuan Bajo Bergetar (CNN)\n",
      "Processing: Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang (CNN)\n",
      "Skipping article (no matching keywords): Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang\n",
      "Processing: Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar (CNN)\n",
      "Skipping article (no matching keywords): Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar\n",
      "Processing: TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara (CNN)\n",
      "Skipping article (no matching keywords): TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara\n",
      "Processing: Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar (CNN)\n",
      "Skipping article (no matching keywords): Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar\n",
      "Processing: Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah (CNN)\n",
      "Skipping article (no matching keywords): Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah\n",
      "Skipping article (contains 'FOTO'): FOTO: Banjir Rendam Puluhan Rumah di Jambi\n",
      "Processing: Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam (CNN)\n",
      "Skipping article (no matching keywords): Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam\n",
      "Processing: Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi (CNN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping article (no matching keywords): Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi\n",
      "Found 1 new articles in CNN. Updating files...\n",
      "File data_cnn.xlsx updated with 1 new articles from CNN.\n",
      "Gabungan file (data_gabungan.xlsx) telah diperbarui dengan 1 artikel baru.\n",
      "\n",
      "========== Scraping Completed. Starting Prediction Process ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction process completed. Data prediksi telah diperbarui.\n",
      "\n",
      "========== Cycle Completed. Waiting 0 minutes before next cycle ==========\n",
      "\n",
      "\n",
      "========== Starting New Scraping Cycle ==========\n",
      "\n",
      "\n",
      "Scraping Detik...\n",
      "Processing: Deretan Foto Fenomena Cuaca Mengerikan, Inikah Kiamat? (Detik)\n",
      "Processing: Penghuni Laut Dalam Ikan Anglerfish Muncul ke Permukaan, Apakah Akan Ada Bencana? (Detik)\n",
      "Processing: 33 Rumah di Tasikmalaya Rusak Akibat Pergeseran Tanah (Detik)\n",
      "No new articles found from Detik.\n",
      "\n",
      "Scraping Kompas...\n",
      "Processing: Indonesia Rawan Bencana, Ikatan Ahli Dorong Pemerintah Bentuk UU Geologi (Kompas)\n",
      "Processing: Bencana Pergeseran Tanah, 15 Rumah di Sumbawa Segera Direlokasi (Kompas)\n",
      "Processing: Pergerakan Tanah Bikin Warga Cikondang Tasikmalaya Resah jika Hujan Mengguyur Desa... (Kompas)\n",
      "Processing: Pergerakan Tanah di Tasikmalaya Meluas, 44 Rumah Terdampak, Retak-retak (Kompas)\n",
      "Processing: Ancaman Tanah Bergerak di Pasuruan: Mengapa Relokasi Jadi Solusi Terbaik? (Kompas)\n",
      "Processing: Puluhan Rumah di Tasikmalaya Terdampak Pergerakan Tanah, BPBD Lapor Tim Geologi Bandung (Kompas)\n",
      "Processing: Efisiensi Anggaran, BPBD Semarang Harap Dana Logistik Bencana Tak Dipangkas (Kompas)\n",
      "Processing: 40 Honorer BPBD Jember Dibebastugaskan, Bisa Tetap Bekerja tapi Tak Ada Gaji (Kompas)\n",
      "Processing: Bisnis Mikro Bisa Ambruk Kapan Saja, BRI Insurance Tawarkan Perlindungan Murah (Kompas)\n",
      "No new articles found from Kompas.\n",
      "\n",
      "Scraping CNN Indonesia...\n",
      "Processing: Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang (CNN)\n",
      "Skipping article (no matching keywords): Truk Angkut 32 Karyawan Terjun ke Sungai di Riau: 4 Tewas, 11 Hilang\n",
      "Processing: Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar (CNN)\n",
      "Skipping article (no matching keywords): Sukatani Manggung di Tegal, Polisi Persilakan Nyanyi Bayar Bayar Bayar\n",
      "Processing: TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara (CNN)\n",
      "Skipping article (no matching keywords): TNI Kerahkan 100 Prajurit Khusus Jaga Keamanan Kota Nusantara\n",
      "Processing: Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar (CNN)\n",
      "Skipping article (no matching keywords): Tiga Kepala Daerah Peserta Retreat Akmil Dilarikan ke RSU Tidar\n",
      "Processing: Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah (CNN)\n",
      "Skipping article (no matching keywords): Kepala Daerah Awali Hari Ketiga Retret Pembekalan dengan Ibadah\n",
      "Skipping article (contains 'FOTO'): FOTO: Banjir Rendam Puluhan Rumah di Jambi\n",
      "Processing: Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam (CNN)\n",
      "Skipping article (no matching keywords): Gibran Blusukan Bareng Wakil Wali Kota Solo Bagi Kemeja Malam-malam\n",
      "Processing: Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi (CNN)\n",
      "Skipping article (no matching keywords): Wagub Rano Ungkap Wacana Bangun Pengolahan Sampah RDF Plant Lagi\n",
      "No new articles found from CNN.\n",
      "\n",
      "========== Scraping Completed. Starting Prediction Process ==========\n",
      "\n",
      "Prediction process completed. Data prediksi telah diperbarui.\n",
      "\n",
      "========== Cycle Completed. Waiting 0 minutes before next cycle ==========\n",
      "\n",
      "\n",
      "Scraping and prediction stopped by user. Exiting safely.\n"
     ]
    }
   ],
   "source": [
    "run_scrape_predict(interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Type</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7791405/k...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Kecamatan Cijeungjing, Kabupaten Ciamis</td>\n",
       "      <td>22 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Sebanyak 7 bangunan terdiri dari rumah warga d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7786842/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Bulukumba, Sulawesi Selatan, Sulsel</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>14.30 WITA</td>\n",
       "      <td>Sebanyak 14 unit rumah mengalami kerusakan aki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.detik.com/edu/detikpedia/d-7786373...</td>\n",
       "      <td>Gempa Bumi</td>\n",
       "      <td>Jepang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.detik.com/jatim/berita/d-7782941/a...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Sidoarjo</td>\n",
       "      <td>17 February 2025</td>\n",
       "      <td>14.30 WIB</td>\n",
       "      <td>Akibatnya, puluhan rumah warga mengalami kerus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7780631/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Banggai, Sulawesi Tengah, Sulteng</td>\n",
       "      <td>16 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Asrama Polsek Balantak di Kabupaten Banggai, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7780166/h...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kecamatan Sukaraja, Kabupaten Tasikmalaya, Jaw...</td>\n",
       "      <td>15 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Di Desa Leuwibudah dua rumah tertimpa pohon se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.detik.com/bali/nusra/d-7774720/fen...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Desa Tangkampulit, Kecamatan Batulanteh, Kabup...</td>\n",
       "      <td>12 February 2025</td>\n",
       "      <td>11.00 WITA</td>\n",
       "      <td>Sebanyak 15 rumah warga terdampak fenomena tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.detik.com/sulsel/berita/d-7772781/...</td>\n",
       "      <td>Puting Beliung</td>\n",
       "      <td>Kabupaten Bulukumba, Sulawesi Selatan, Sulsel</td>\n",
       "      <td>11 February 2025</td>\n",
       "      <td>14.30 WITA</td>\n",
       "      <td>Sebanyak 15 unit rumah dan 1 masjid di Kabupat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/20/171...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Flores Timur, Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>21 December 2024</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://surabaya.kompas.com/read/2025/02/20/110...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Kabupaten Lumajang, Jawa Timur</td>\n",
       "      <td>20 February 2025</td>\n",
       "      <td>00.00 WIB</td>\n",
       "      <td>Sebanyak empat kali erupsi terpantau jelas sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://denpasar.kompas.com/read/2025/02/19/160...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Banjar Dinas Suci, Desa Tejakula, Kecamatan Te...</td>\n",
       "      <td>05 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Krisis tersebut akibat kerusakan pipa induk im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/18/201...</td>\n",
       "      <td>Gunung Meletus</td>\n",
       "      <td>Timur, Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>12 February 2025</td>\n",
       "      <td>03.00 WITA</td>\n",
       "      <td>Penjabat Bupati Flores Timur, Nusa Tenggara Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://regional.kompas.com/read/2025/02/14/170...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Nusa Tenggara Timur, Ntt</td>\n",
       "      <td>14 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Kejaksaan Negeri Lembata , Nusa Tenggara Timur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://money.kompas.com/read/2025/02/14/073000...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Desa Sungai Itik, Kecamatan Sadu, Kabupaten Ta...</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>Terbaru, PetroChina menyalurkan bantuan bagi k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Desa Sisundung, Kecamatan Angkola Barat, Kabup...</td>\n",
       "      <td>22 February</td>\n",
       "      <td>18.00 WIB</td>\n",
       "      <td>Tanah longsor yang terjadi di Desa Sisundung, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Kota Bandar Lampung</td>\n",
       "      <td>23 February</td>\n",
       "      <td>00.51 WIB</td>\n",
       "      <td>Kementerian Sosial (Kemensos) mengirimkan bant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250223...</td>\n",
       "      <td>Banjir</td>\n",
       "      <td>Jakarta Utara</td>\n",
       "      <td>23 February</td>\n",
       "      <td>Tidak ada dalam artikel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.detik.com/jabar/berita/d-7792142/l...</td>\n",
       "      <td>Tanah Longsor</td>\n",
       "      <td>Bungbulang, Garut</td>\n",
       "      <td>23 February 2025</td>\n",
       "      <td>18.30 WIB</td>\n",
       "      <td>Hingga Minggu malam pukul 20.46 WIB ini, korba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20250224...</td>\n",
       "      <td>Gempa Bumi</td>\n",
       "      <td>Waingapu, Sumba Timur, Ntt</td>\n",
       "      <td>24 February</td>\n",
       "      <td>02.21 WIB</td>\n",
       "      <td>[Gambas:Twitter] Hingga saat ini belum diketah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL            Type  \\\n",
       "0   https://www.detik.com/jabar/berita/d-7791405/k...          Banjir   \n",
       "1   https://www.detik.com/sulsel/berita/d-7786842/...  Puting Beliung   \n",
       "2   https://www.detik.com/edu/detikpedia/d-7786373...      Gempa Bumi   \n",
       "3   https://www.detik.com/jatim/berita/d-7782941/a...  Puting Beliung   \n",
       "4   https://www.detik.com/sulsel/berita/d-7780631/...  Puting Beliung   \n",
       "5   https://www.detik.com/jabar/berita/d-7780166/h...  Puting Beliung   \n",
       "6   https://www.detik.com/bali/nusra/d-7774720/fen...   Tanah Longsor   \n",
       "7   https://www.detik.com/sulsel/berita/d-7772781/...  Puting Beliung   \n",
       "8   http://regional.kompas.com/read/2025/02/20/171...  Gunung Meletus   \n",
       "9   http://surabaya.kompas.com/read/2025/02/20/110...  Gunung Meletus   \n",
       "10  http://denpasar.kompas.com/read/2025/02/19/160...          Banjir   \n",
       "11  http://regional.kompas.com/read/2025/02/18/201...  Gunung Meletus   \n",
       "12  http://regional.kompas.com/read/2025/02/14/170...          Banjir   \n",
       "13  http://money.kompas.com/read/2025/02/14/073000...          Banjir   \n",
       "14  https://www.cnnindonesia.com/nasional/20250223...   Tanah Longsor   \n",
       "15  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "16  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "17  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "18  https://www.cnnindonesia.com/nasional/20250223...          Banjir   \n",
       "19  https://www.detik.com/jabar/berita/d-7792142/l...   Tanah Longsor   \n",
       "20  https://www.cnnindonesia.com/nasional/20250224...      Gempa Bumi   \n",
       "\n",
       "                                             Location              Date  \\\n",
       "0             Kecamatan Cijeungjing, Kabupaten Ciamis  22 February 2025   \n",
       "1       Kabupaten Bulukumba, Sulawesi Selatan, Sulsel  20 February 2025   \n",
       "2                                              Jepang               NaN   \n",
       "3                                  Kabupaten Sidoarjo  17 February 2025   \n",
       "4         Kabupaten Banggai, Sulawesi Tengah, Sulteng  16 February 2025   \n",
       "5   Kecamatan Sukaraja, Kabupaten Tasikmalaya, Jaw...       15 February   \n",
       "6   Desa Tangkampulit, Kecamatan Batulanteh, Kabup...  12 February 2025   \n",
       "7       Kabupaten Bulukumba, Sulawesi Selatan, Sulsel  11 February 2025   \n",
       "8              Flores Timur, Nusa Tenggara Timur, Ntt  21 December 2024   \n",
       "9                      Kabupaten Lumajang, Jawa Timur  20 February 2025   \n",
       "10  Banjar Dinas Suci, Desa Tejakula, Kecamatan Te...  05 February 2025   \n",
       "11                    Timur, Nusa Tenggara Timur, Ntt  12 February 2025   \n",
       "12                           Nusa Tenggara Timur, Ntt  14 February 2025   \n",
       "13  Desa Sungai Itik, Kecamatan Sadu, Kabupaten Ta...  13 February 2025   \n",
       "14  Desa Sisundung, Kecamatan Angkola Barat, Kabup...       22 February   \n",
       "15                                Kota Bandar Lampung       23 February   \n",
       "16                                      Jakarta Utara       23 February   \n",
       "17                                      Jakarta Utara       23 February   \n",
       "18                                      Jakarta Utara       23 February   \n",
       "19                                  Bungbulang, Garut  23 February 2025   \n",
       "20                         Waingapu, Sumba Timur, Ntt       24 February   \n",
       "\n",
       "                       Time                                             Impact  \n",
       "0   Tidak ada dalam artikel  Sebanyak 7 bangunan terdiri dari rumah warga d...  \n",
       "1                14.30 WITA  Sebanyak 14 unit rumah mengalami kerusakan aki...  \n",
       "2   Tidak ada dalam artikel                                                NaN  \n",
       "3                 14.30 WIB  Akibatnya, puluhan rumah warga mengalami kerus...  \n",
       "4   Tidak ada dalam artikel  Asrama Polsek Balantak di Kabupaten Banggai, S...  \n",
       "5   Tidak ada dalam artikel  Di Desa Leuwibudah dua rumah tertimpa pohon se...  \n",
       "6                11.00 WITA  Sebanyak 15 rumah warga terdampak fenomena tan...  \n",
       "7                14.30 WITA  Sebanyak 15 unit rumah dan 1 masjid di Kabupat...  \n",
       "8   Tidak ada dalam artikel                                                NaN  \n",
       "9                 00.00 WIB  Sebanyak empat kali erupsi terpantau jelas sec...  \n",
       "10  Tidak ada dalam artikel  Krisis tersebut akibat kerusakan pipa induk im...  \n",
       "11               03.00 WITA  Penjabat Bupati Flores Timur, Nusa Tenggara Ti...  \n",
       "12  Tidak ada dalam artikel  Kejaksaan Negeri Lembata , Nusa Tenggara Timur...  \n",
       "13  Tidak ada dalam artikel  Terbaru, PetroChina menyalurkan bantuan bagi k...  \n",
       "14                18.00 WIB  Tanah longsor yang terjadi di Desa Sisundung, ...  \n",
       "15                00.51 WIB  Kementerian Sosial (Kemensos) mengirimkan bant...  \n",
       "16  Tidak ada dalam artikel                                                NaN  \n",
       "17  Tidak ada dalam artikel                                                NaN  \n",
       "18  Tidak ada dalam artikel                                                NaN  \n",
       "19                18.30 WIB  Hingga Minggu malam pukul 20.46 WIB ini, korba...  \n",
       "20                02.21 WIB  [Gambas:Twitter] Hingga saat ini belum diketah...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
